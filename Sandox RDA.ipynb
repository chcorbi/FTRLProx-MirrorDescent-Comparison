{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logloss_fun(x, y):\n",
    "    return lambda theta: np.log(1 + np.exp(-y * x.dot(theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_logloss_fun(x, y):\n",
    "    return lambda theta: 1 / (1 + np.exp(y * x.dot(theta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "\n",
    "def loading_dataset(filename):\n",
    "    data = load_svmlight_file(filename)\n",
    "    return data[0], data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = loading_dataset('Datasets/rcv1_train.binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnz_entries = np.unique(X.nonzero()[0])\n",
    "X = X[nnz_entries]\n",
    "y = y[nnz_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SqrtIterator():\n",
    "    import numpy as np\n",
    "    \n",
    "    def __init__(self, gamma=1.0):\n",
    "        self._t = 1\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        sq = np.sqrt(self._t)\n",
    "        self._t += 1\n",
    "        return self.gamma * sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.41421356237\n",
      "1.73205080757\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "sqrt_iter = SqrtIterator()\n",
    "for _ in range(4):\n",
    "    print(next(sqrt_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RDASolver(BaseEstimator):\n",
    "    def __init__(self, lbda, gamma):\n",
    "        \"\"\"Implement the Regularized Dual Avering method [Xiao 2009] (cf. Algorithm 1)\n",
    "\n",
    "        Args:\n",
    "            w_dim (int): the dimension of the solution vector\n",
    "            psi (func): penalization function (input: vector of shape (w_dim, ), output: float)\n",
    "            aux (func): auxiliary function defined in the paper\n",
    "            betas (iterator): non-negative non-decreasing series of float numbers\n",
    "        \"\"\"\n",
    "        self.lbda = lbda\n",
    "        self.gamma = gamma\n",
    "        self._t = 1\n",
    "        self._gBar = None\n",
    "        self.betas = SqrtIterator(gamma)\n",
    "        self.w = None\n",
    "        self.losses = []\n",
    "        self.log_likelihood = 0\n",
    "\n",
    "    def iterate(self, X, y, g):\n",
    "        # We compute the average of the past gradients\n",
    "        self._gBar = (self._t - 1) / self._t * self._gBar + 1 / self._t * g\n",
    "\n",
    "        # We can then define the objective function to minimize\n",
    "        beta_t = next(self.betas)\n",
    "        \n",
    "        nnz_X = X.nonzero()[1]\n",
    "        if nnz_X.size == 0:\n",
    "            raise \"Error at ligne %d \" % (self._t + 1)\n",
    "        \n",
    "        # Update weight\n",
    "        for i in nnz_X:\n",
    "            if self._gBar[0, i] <= self.lbda:\n",
    "                self.w[0, i] = 0\n",
    "            else:\n",
    "                sign = 1. if self._gBar[i] >= 0 else -1.\n",
    "                w[0, i] = - beta_t / self.gamma * (self._gBar[0, i] - self.lbda * sign)\n",
    "        \n",
    "        # Compute probability and losses\n",
    "        wtx = self.w.dot(X.T)\n",
    "        p = sigmoid(wtx)\n",
    "        self.log_likelihood += log_loss(y, p)\n",
    "        self.losses.append(self.log_likelihood)\n",
    "        \n",
    "        self._t += 1\n",
    "        return self.w, p\n",
    "\n",
    "    def status(self):\n",
    "        data = {\"t\": self._t, \"w\": self.w, \"gBar\": self._gBar}\n",
    "        return data\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        start_time = time.time()\n",
    "\n",
    "        w_dim = X.shape[1]\n",
    "        self.w = csr_matrix((1, w_dim))\n",
    "        self._gBar = np.zeros((1, w_dim))\n",
    "        y_proba = []\n",
    "\n",
    "        for t in range(X.shape[0]):\n",
    "            g = X[t] / (1 + np.exp(-y[t] * self.w.dot(X[t].T)[0, 0]))\n",
    "            w, p = self.iterate(X[t], y[t], g)\n",
    "            y_proba.append(p)\n",
    "            if t % int(X.shape[0] / 10) == 0:\n",
    "                print(\"Training Samples: %d |\\t Loss: %s\" % (t, self.log_likelihood))\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rda = RDASolver(gamma=1.0, lbda=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000\n",
    "np.random.seed(42)\n",
    "i = np.random.choice(np.arange(X.shape[0]), N, replace=False)\n",
    "X_sub = X[i]\n",
    "y_sub = y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/camille/.virtualenvs/ds/lib/python3.5/site-packages/scipy/sparse/compressed.py:730: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 0 |\t Loss: [[ 0.69314718]]\n",
      "Training Samples: 100 |\t Loss: [[ 70.00786524]]\n",
      "Training Samples: 200 |\t Loss: [[ 139.32258329]]\n",
      "Training Samples: 300 |\t Loss: [[ 208.63730135]]\n",
      "Training Samples: 400 |\t Loss: [[ 277.9520194]]\n",
      "Training Samples: 500 |\t Loss: [[ 347.26673746]]\n",
      "Training Samples: 600 |\t Loss: [[ 416.58145552]]\n",
      "Training Samples: 700 |\t Loss: [[ 485.89617357]]\n",
      "Training Samples: 800 |\t Loss: [[ 555.21089163]]\n",
      "Training Samples: 900 |\t Loss: [[ 624.52560968]]\n"
     ]
    }
   ],
   "source": [
    "rda.fit(X_sub, y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
